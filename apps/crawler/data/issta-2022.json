{
    "long_title": null,
    "short_title": "ISSTA",
    "year": 2022,
    "location": "Online",
    "from_date": "2022-05-18T00:00:00",
    "to_date": "2022-07-22T00:00:00",
    "important_dates": [
        {
            "date": "2022-01-28T00:00:00",
            "description": "Papers due"
        },
        {
            "date": "2022-03-25T00:00:00",
            "description": "Author response"
        },
        {
            "date": "2022-04-11T00:00:00",
            "description": "Paper notification"
        },
        {
            "date": "2022-05-27T00:00:00",
            "description": "Camera-ready papers due"
        }
    ],
    "url": "https://conf.researchr.org/home/issta-2022",
    "call_for_papers": "## Call for Papers\n\n[Updated Jan. 2022]\n\n\nISSTA invites three kinds of submissions. The clear majority of submissions is expected to be \u201cResearch Papers\u201d, but submissions that best fit the description of \u201cExperience Papers\u201d or \u201cReplicability Studies\u201d should be submitted as such.\n\n\n### [Research Papers](#research-papers)\n\n\nAuthors are invited to submit research papers describing original contributions in testing or analysis of computer software. Papers describing original theoretical or empirical research, new techniques, methods for emerging systems, in-depth case studies, infrastructures of testing and analysis, or tools are welcome.\n\n\n### [Experience Papers](#experience-papers)\n\n\nAuthors are invited to submit experience papers describing a significant experience in applying software testing and analysis methods or tools and should carefully identify and discuss important lessons learned so that other researchers and/or practitioners can benefit from the experience. Of special interest are experience papers that report on industrial applications of software testing and analysis methods or tools.\n\n\n### [Replicability Studies](#replicability-studies)\n\n\nISSTA would like to encourage researchers to replicate results from previous papers. A replicability study must go beyond simply re-implementing an algorithm and/or re-running the artifacts provided by the original paper. It should at the very least apply the approach to new, significantly broadened inputs. Particularly, replicability studies are encouraged to target techniques that previously were evaluated only on proprietary subject programs or inputs. A replicability study should clearly report on results that the authors were able to replicate as well as on aspects of the work that were not replicable. In the latter case, authors are encouraged to make an effort to communicate or collaborate with the original paper\u2019s authors to determine the cause for any observed discrepancies and, if possible, address them (e.g., through minor implementation changes). We explicitly encourage authors to not focus on a single paper/artifact only, but instead to perform a comparative experiment of multiple related approaches.\n\n\nIn particular, replicability studies should follow the ACM guidelines on replicability (different team, different experimental setup): The measurement can be obtained with stated precision by a different team, a different measuring system, in a different location on multiple trials. For computational experiments, this means that an independent group can obtain the same result using artifacts which they develop completely independently. This means that it is also insufficient to focus on reproducibility (i.e., different team, same experimental setup) alone. Replicability Studies will be evaluated according to the following standards:\n\n\n* Depth and breadth of experiments\n* Clarity of writing\n* Appropriateness of conclusions\n* Amount of useful, actionable insights\n* Availability of artifacts\n\n\nWe expect replicability studies to clearly point out the artifacts the study is built on, and to submit those artifacts to artifact evaluation (see below). Artifacts evaluated positively will be eligible to obtain the prestigious Results Reproduced badge.\n\n\n## [Submission Guidelines](#submission-guidelines)\n\n\nSubmissions must be original and should not have been published previously or be under consideration for publication while being evaluated for ISSTA. Authors are required to adhere to the ACM Policy and Procedures on Plagiarism and the ACM Policy on Prior Publication and Simultaneous Submissions. More details are available at the Submission Policies page.\n\n\nResearch and Experience Papers as well as Replicability Studies should be at most 10 pages in length, with at most 2 additional pages for references. Experience papers and replicability studies should clearly specify their category in the paper title upon submission, e.g., \u201cXXX (Experience Paper)\u201d. All authors should use the official \u201cACM Master article template\u201d, which can be obtained from [the ACM Proceedings Template pages](https://www.acm.org/publications/proceedings-template). Latex users should use the \u201csigconf\u201d option, as well as the \u201creview\u201d (to produce line numbers for easy reference by the reviewers) and \u201canonymous\u201d (omitting author names) options. To that end, the following latex code can be placed at the start of the latex document:\n\n\n\n```\n\\documentclass[sigconf,review, anonymous]{acmart}\n\\acmConference[ISSTA 2022]{ACM SIGSOFT International Symposium on Software Testing and Analysis}{18-22 July, 2022}{Daejeon, South Korea}\n\n```\n\nSubmit your papers via the [HotCRP ISSTA 2022 submission website](https://issta22.hotcrp.com).\n\n\n#### [Double-blind Reviewing](#double-blind-reviewing)\n\n\nISSTA 2022 will conduct double-blind reviewing. Submissions should not reveal the identity of the authors in any way. Authors should leave out author names and affiliations from the body of their submission. They should also ensure that any citations to related work by themselves are written in third person, that is, \u201cthe prior work of XYZ\u201d as opposed to \u201cour prior work\u201d.\n\n\nDouble-blind reviewing should not hinder the usual communication of results. But, during the review period, please don\u2019t broadcast the work on social media. Also, to the extent possible, consider not publishing preprints of your work during or right before the review period.\n\n\nAuthors with further questions on double-blind reviewing are encouraged to contact the Program Chair by email.\n\n\n#### [Supplementary Material](#supplementary-material)\n\n\nAuthors are free to provide supplementary material if that material supports the claims in the paper. Such material may include proofs, experimental results, and/or data sets. This material should be uploaded at the same time as the submission. Any supplementary material must also be anonymized. Reviewers are not required to examine the supplementary material but may refer to it if they would like to find further evidence supporting the claims in the paper. Please prepare the supplementary material in the PDF format.\n\n\n#### [Author Responses](#author-responses)\n\n\nReviews will be followed by an author response. After the response, some papers may receive additional reviews, if necessary, to which authors can respond in a second author-response phase.\n\n\n#### [Visa Policy](#visa-policy)\n\n\nIn cases where the travel Visa is rejected or delayed for a long time (e.g., due to additional checks) so that the authors are not able to attend the conference despite best efforts, alternative presentation methods will be considered. However, the authors should notify the program chair at least one week before the conference and provide proof of visa being applied in time.\n\n\n"
}